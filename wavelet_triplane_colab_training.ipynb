{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033ee588",
   "metadata": {},
   "source": [
    "## üì¶ B∆∞·ªõc 1: C√†i ƒê·∫∑t v√† Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404655c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b11118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install einops tqdm matplotlib pillow\n",
    "!pip install trimesh[easy] pytorch3d objaverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda64389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive ƒë·ªÉ l∆∞u model\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c l∆∞u model\n",
    "!mkdir -p /content/drive/MyDrive/WaveletTriplane\n",
    "\n",
    "SAVE_DIR = '/content/drive/MyDrive/WaveletTriplane'\n",
    "print(f\"Model s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae1cf2",
   "metadata": {},
   "source": [
    "## üì• B∆∞·ªõc 2: T·∫£i Dataset Th·∫≠t\n",
    "\n",
    "**L·ª±a ch·ªçn dataset (ch·ªçn 1 trong 2):**\n",
    "\n",
    "### **Option A: ShapeNet Chairs** ‚≠ê Recommended\n",
    "- üì¶ Size: ~2GB (6,778 models)\n",
    "- ‚è∞ Th·ªùi gian t·∫£i: ~5-10 ph√∫t\n",
    "- üéØ Ch·∫•t l∆∞·ª£ng: Cao, chu·∫©n h√≥a t·ªët\n",
    "- üí° T·ªët nh·∫•t cho training nhanh\n",
    "\n",
    "### **Option B: Objaverse**\n",
    "- üì¶ Size: ~800MB cho 100 objects\n",
    "- ‚è∞ Th·ªùi gian t·∫£i: ~10-15 ph√∫t\n",
    "- üéØ Ch·∫•t l∆∞·ª£ng: R·∫•t ƒëa d·∫°ng\n",
    "- üí° T·ªët cho model t·ªïng qu√°t\n",
    "\n",
    "**‚ö†Ô∏è L∆∞u √Ω:** Ch·ªâ ch·∫°y 1 trong 2 cell d∆∞·ªõi ƒë√¢y!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Download ShapeNet subset t·ª´ Hugging Face\n",
    "import os\n",
    "import gdown\n",
    "\n",
    "DATASET_DIR = '/content/shapenet_data'\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üì• Downloading ShapeNet Chair dataset from Google Drive...\")\n",
    "\n",
    "# ShapeNet Chairs subset (~2GB, 6778 models)\n",
    "# Link public t·ª´ c√°c ngu·ªìn m·ªü\n",
    "url = \"https://drive.google.com/uc?id=1Z8gt4HdPujBNFABYrthhau9VZW10WWYe\"\n",
    "\n",
    "try:\n",
    "    # Download v√† extract\n",
    "    gdown.download(url, f'{DATASET_DIR}/shapenet_chairs.zip', quiet=False)\n",
    "    \n",
    "    # Extract\n",
    "    import zipfile\n",
    "    print(\"üì¶ Extracting dataset...\")\n",
    "    with zipfile.ZipFile(f'{DATASET_DIR}/shapenet_chairs.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATASET_DIR)\n",
    "    \n",
    "    print(\"‚úì ShapeNet dataset downloaded successfully!\")\n",
    "    !ls -la {DATASET_DIR}\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è L·ªói download: {e}\")\n",
    "    print(\"‚ö†Ô∏è S·ª≠ d·ª•ng dataset demo nh·ªè thay th·∫ø...\")\n",
    "    \n",
    "    # Fallback: T·∫£i dataset nh·ªè h∆°n t·ª´ ModelNet10\n",
    "    print(\"\\nüì• Downloading ModelNet10 (backup dataset - 48MB)...\")\n",
    "    !wget http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip -O {DATASET_DIR}/modelnet.zip\n",
    "    !unzip -q {DATASET_DIR}/modelnet.zip -d {DATASET_DIR}\n",
    "    print(\"‚úì ModelNet10 dataset ready!\")\n",
    "    DATASET_DIR = f'{DATASET_DIR}/ModelNet10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc4a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Download Objaverse samples (t·ª± ƒë·ªông t·∫£i t·ª´ server ch√≠nh th·ª©c)\n",
    "import objaverse\n",
    "\n",
    "print(\"üì• Downloading Objaverse objects (official source)...\")\n",
    "print(\"‚è∞ Th·ªùi gian: ~10-15 ph√∫t cho 100 objects\")\n",
    "\n",
    "# T·∫£i 100 objects ch·∫•t l∆∞·ª£ng cao t·ª´ Objaverse\n",
    "uids = objaverse.load_uids()[:100]  # L·∫•y 100 objects ƒë·∫ßu ti√™n\n",
    "\n",
    "objects = objaverse.load_objects(\n",
    "    uids=uids,\n",
    "    download_processes=4\n",
    ")\n",
    "\n",
    "print(f\"‚úì Downloaded {len(objects)} objects\")\n",
    "print(f\"üìÇ Location: {objaverse._VERSIONED_PATH}\")\n",
    "DATASET_DIR = objaverse._VERSIONED_PATH\n",
    "\n",
    "# List m·ªôt v√†i objects\n",
    "import os\n",
    "obj_files = list(objects.values())[:5]\n",
    "print(f\"\\nüìã Sample objects:\")\n",
    "for obj in obj_files:\n",
    "    print(f\"  - {os.path.basename(obj)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7bf89f",
   "metadata": {},
   "source": [
    "## üèóÔ∏è B∆∞·ªõc 3: Copy Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc40f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t th√™m rendering tools\n",
    "!pip install -q pyrender trimesh pyglet\n",
    "\n",
    "import trimesh\n",
    "import pyrender\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def render_mesh_multiview(mesh_path, output_dir, num_views=8, resolution=256):\n",
    "    \"\"\"\n",
    "    Render 3D mesh th√†nh nhi·ªÅu views.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load mesh\n",
    "        mesh = trimesh.load(mesh_path, force='mesh')\n",
    "        \n",
    "        # Center v√† normalize\n",
    "        mesh.vertices -= mesh.vertices.mean(axis=0)\n",
    "        mesh.vertices /= np.abs(mesh.vertices).max()\n",
    "        mesh.vertices *= 0.8\n",
    "        \n",
    "        # Create scene\n",
    "        scene = pyrender.Scene()\n",
    "        mesh_node = scene.add(pyrender.Mesh.from_trimesh(mesh))\n",
    "        \n",
    "        # Camera\n",
    "        camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0)\n",
    "        \n",
    "        # Light\n",
    "        light = pyrender.DirectionalLight(color=np.ones(3), intensity=3.0)\n",
    "        scene.add(light)\n",
    "        \n",
    "        # Renderer\n",
    "        r = pyrender.OffscreenRenderer(resolution, resolution)\n",
    "        \n",
    "        # Render multiple views\n",
    "        views = []\n",
    "        for i in range(num_views):\n",
    "            angle = 2 * np.pi * i / num_views\n",
    "            \n",
    "            # Camera position (orbit around object)\n",
    "            cam_pos = np.array([\n",
    "                2.0 * np.cos(angle),\n",
    "                0.5,\n",
    "                2.0 * np.sin(angle)\n",
    "            ])\n",
    "            \n",
    "            # Look at center\n",
    "            camera_pose = np.eye(4)\n",
    "            camera_pose[:3, 3] = cam_pos\n",
    "            \n",
    "            # Point camera at origin\n",
    "            z = -cam_pos / np.linalg.norm(cam_pos)\n",
    "            x = np.cross([0, 1, 0], z)\n",
    "            x = x / np.linalg.norm(x)\n",
    "            y = np.cross(z, x)\n",
    "            \n",
    "            camera_pose[:3, 0] = x\n",
    "            camera_pose[:3, 1] = y\n",
    "            camera_pose[:3, 2] = z\n",
    "            \n",
    "            scene.add(camera, pose=camera_pose)\n",
    "            \n",
    "            # Render\n",
    "            color, _ = r.render(scene)\n",
    "            views.append(color)\n",
    "            \n",
    "            scene.remove_node(list(scene.get_nodes(obj=camera))[0])\n",
    "        \n",
    "        r.delete()\n",
    "        \n",
    "        # Save views\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        for i, view in enumerate(views):\n",
    "            img = Image.fromarray(view)\n",
    "            img.save(os.path.join(output_dir, f'view_{i}.png'))\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering {mesh_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Render dataset\n",
    "print(\"üé® Rendering 3D models to multi-view images...\")\n",
    "print(\"‚è∞ Th·ªùi gian: ~1-2 ph√∫t cho 100 models\\n\")\n",
    "\n",
    "RENDERED_DIR = '/content/rendered_dataset'\n",
    "os.makedirs(RENDERED_DIR, exist_ok=True)\n",
    "\n",
    "# T√¨m t·∫•t c·∫£ file .obj v√† .glb\n",
    "mesh_files = []\n",
    "for ext in ['*.obj', '*.glb', '*.ply']:\n",
    "    mesh_files.extend(list(Path(DATASET_DIR).rglob(ext)))\n",
    "\n",
    "# Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ demo nhanh\n",
    "max_models = min(100, len(mesh_files))\n",
    "mesh_files = mesh_files[:max_models]\n",
    "\n",
    "print(f\"Found {len(mesh_files)} 3D models\")\n",
    "print(f\"Rendering {max_models} models...\\n\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "success_count = 0\n",
    "\n",
    "for i, mesh_path in enumerate(tqdm(mesh_files[:max_models])):\n",
    "    output_dir = os.path.join(RENDERED_DIR, f'object_{i:04d}')\n",
    "    \n",
    "    if render_mesh_multiview(str(mesh_path), output_dir, num_views=8):\n",
    "        success_count += 1\n",
    "\n",
    "print(f\"\\n‚úì Rendered {success_count}/{max_models} models successfully!\")\n",
    "print(f\"üìÇ Saved to: {RENDERED_DIR}\")\n",
    "\n",
    "# Update DATASET_DIR\n",
    "DATASET_DIR = RENDERED_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ff0ee",
   "metadata": {},
   "source": [
    "## üé® B∆∞·ªõc 3.5: Render 3D Models th√†nh Images\n",
    "\n",
    "**Quan tr·ªçng:** Dataset 3D c·∫ßn render th√†nh multi-view images tr∆∞·ªõc khi train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone code t·ª´ GitHub repo\n",
    "!git clone https://github.com/HoangNguyennnnnnn/WaveletTriplaneDiff.git\n",
    "!cd WaveletTriplaneDiff && ls -la\n",
    "\n",
    "# Copy file c·∫ßn thi·∫øt ra th∆∞ m·ª•c ch√≠nh\n",
    "!cp WaveletTriplaneDiff/wavelet_triplane_diffusion.py .\n",
    "\n",
    "print(\"‚úì ƒê√£ clone repo th√†nh c√¥ng!\")\n",
    "print(\"‚úì File wavelet_triplane_diffusion.py ƒë√£ s·∫µn s√†ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa7b1c",
   "metadata": {},
   "source": [
    "## üéØ B∆∞·ªõc 4: T·∫°o Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db63c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MultiViewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset cho 3D objects v·ªõi multi-view rendering.\n",
    "    Load images th·∫≠t t·ª´ th∆∞ m·ª•c ƒë√£ render.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, num_views=4, image_size=256):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.num_views = num_views\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Transform v·ªõi data augmentation\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.3),  # Augmentation\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Augmentation\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        self.transform_target = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        # T√¨m t·∫•t c·∫£ objects\n",
    "        self.objects = []\n",
    "        if os.path.exists(data_dir):\n",
    "            for obj_dir in Path(data_dir).iterdir():\n",
    "                if obj_dir.is_dir():\n",
    "                    # Ki·ªÉm tra c√≥ view_0.png kh√¥ng\n",
    "                    if (obj_dir / 'view_0.png').exists():\n",
    "                        self.objects.append(obj_dir)\n",
    "        \n",
    "        if len(self.objects) == 0:\n",
    "            print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y rendered images!\")\n",
    "            print(\"‚ö†Ô∏è Vui l√≤ng ch·∫°y cell render 3D models tr∆∞·ªõc!\")\n",
    "            raise ValueError(\"No rendered dataset found!\")\n",
    "        else:\n",
    "            print(f\"‚úì T√¨m th·∫•y {len(self.objects)} objects v·ªõi multi-view images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.objects)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obj_dir = self.objects[idx]\n",
    "        \n",
    "        # Load input view (view 0)\n",
    "        input_path = obj_dir / 'view_0.png'\n",
    "        input_view = Image.open(input_path).convert('RGB')\n",
    "        input_view = self.transform(input_view)\n",
    "        \n",
    "        # Load target views (views 1-4)\n",
    "        target_views = []\n",
    "        for i in range(1, self.num_views + 1):\n",
    "            view_path = obj_dir / f'view_{i}.png'\n",
    "            \n",
    "            if view_path.exists():\n",
    "                view = Image.open(view_path).convert('RGB')\n",
    "                view = self.transform_target(view)\n",
    "            else:\n",
    "                # N·∫øu thi·∫øu view, d√πng view 0\n",
    "                view = Image.open(input_path).convert('RGB')\n",
    "                view = self.transform_target(view)\n",
    "            \n",
    "            target_views.append(view)\n",
    "        \n",
    "        target_views = torch.stack(target_views)\n",
    "        \n",
    "        return {\n",
    "            'image': input_view,\n",
    "            'target_views': target_views,\n",
    "        }\n",
    "\n",
    "# Test dataset\n",
    "print(\"\\nüìä Testing dataset loader...\")\n",
    "dataset = MultiViewDataset(DATASET_DIR)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "sample = dataset[0]\n",
    "print(f\"Input shape: {sample['image'].shape}\")\n",
    "print(f\"Target views shape: {sample['target_views'].shape}\")\n",
    "\n",
    "# Visualize m·ªôt sample\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "# Input\n",
    "img = sample['image'].permute(1, 2, 0).numpy()\n",
    "img = (img * 0.5 + 0.5).clip(0, 1)\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title('Input View')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Target views\n",
    "for i in range(4):\n",
    "    img = sample['target_views'][i].permute(1, 2, 0).numpy()\n",
    "    img = (img * 0.5 + 0.5).clip(0, 1)\n",
    "    axes[i+1].imshow(img)\n",
    "    axes[i+1].set_title(f'Target {i+1}')\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, 'dataset_sample.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da7b13",
   "metadata": {},
   "source": [
    "## üèãÔ∏è B∆∞·ªõc 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from wavelet_triplane_diffusion import WaveletTriplaneDiffusion\n",
    "\n",
    "# Training config\n",
    "CONFIG = {\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'triplane_channels': 32,\n",
    "    'num_timesteps': 1000,\n",
    "    'save_every': 5,  # Save checkpoint m·ªói 5 epochs\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beff08e",
   "metadata": {},
   "source": [
    "## üöÄ B∆∞·ªõc 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create dataloader\n",
    "train_dataset = MultiViewDataset(DATASET_DIR)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Create model\n",
    "device = CONFIG['device']\n",
    "model = WaveletTriplaneDiffusion(triplane_channels=CONFIG['triplane_channels'])\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['num_epochs'])\n",
    "\n",
    "# Loss tracking\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function v·ªõi perceptual loss\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mse = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        images = batch['image'].to(device)\n",
    "        target_views = batch['target_views'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, return_intermediate=True)\n",
    "        \n",
    "        # Multi-view rendering loss (so s√°nh v·ªõi t·∫•t c·∫£ target views)\n",
    "        rendered = outputs['rendered_image']\n",
    "        \n",
    "        # Loss v·ªõi multiple target views\n",
    "        losses = []\n",
    "        for i in range(target_views.shape[1]):\n",
    "            target = target_views[:, i]\n",
    "            \n",
    "            # MSE loss\n",
    "            mse_loss = F.mse_loss(rendered, target)\n",
    "            \n",
    "            # L1 loss (robust h∆°n)\n",
    "            l1_loss = F.l1_loss(rendered, target)\n",
    "            \n",
    "            # Combined loss\n",
    "            view_loss = 0.7 * mse_loss + 0.3 * l1_loss\n",
    "            losses.append(view_loss)\n",
    "        \n",
    "        # L·∫•y loss nh·ªè nh·∫•t (best matching view)\n",
    "        loss = min(losses)\n",
    "        \n",
    "        # Regularization: triplane smoothness\n",
    "        triplane = outputs['triplane_highres']\n",
    "        smooth_loss = 0.01 * (\n",
    "            F.l1_loss(triplane[:, :, :, :, :-1], triplane[:, :, :, :, 1:]) +\n",
    "            F.l1_loss(triplane[:, :, :, :-1, :], triplane[:, :, :, 1:, :])\n",
    "        )\n",
    "        \n",
    "        total_train_loss = loss + smooth_loss\n",
    "        \n",
    "        # Backward\n",
    "        total_train_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += total_train_loss.item()\n",
    "        total_mse += loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{total_train_loss.item():.4f}',\n",
    "            'mse': f'{loss.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_mse = total_mse / len(dataloader)\n",
    "    \n",
    "    return {'total': avg_loss, 'mse': avg_mse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2733e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "best_loss = float('inf')\n",
    "train_loss_history = []\n",
    "mse_loss_history = []\n",
    "\n",
    "for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "    # Train\n",
    "    losses = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "    avg_loss = losses['total']\n",
    "    avg_mse = losses['mse']\n",
    "    \n",
    "    train_loss_history.append(avg_loss)\n",
    "    mse_loss_history.append(avg_mse)\n",
    "    \n",
    "    # Log\n",
    "    print(f\"\\nEpoch {epoch}/{CONFIG['num_epochs']}\")\n",
    "    print(f\"  Total Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  MSE Loss: {avg_mse:.4f}\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        save_path = os.path.join(SAVE_DIR, 'wavelet_triplane_best.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "            'config': CONFIG,\n",
    "        }, save_path)\n",
    "        print(f\"  ‚úì Saved best model to {save_path}\")\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if epoch % CONFIG['save_every'] == 0:\n",
    "        save_path = os.path.join(SAVE_DIR, f'wavelet_triplane_epoch_{epoch}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'config': CONFIG,\n",
    "        }, save_path)\n",
    "        print(f\"  ‚úì Saved checkpoint to {save_path}\")\n",
    "        \n",
    "        # Visualize progress\n",
    "        if epoch % (CONFIG['save_every'] * 2) == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_sample = train_dataset[0]\n",
    "                test_img = test_sample['image'].unsqueeze(0).to(device)\n",
    "                output = model(test_img, return_intermediate=True)\n",
    "                \n",
    "                fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "                \n",
    "                # Input\n",
    "                img = test_img[0].cpu().permute(1, 2, 0).numpy()\n",
    "                img = (img * 0.5 + 0.5).clip(0, 1)\n",
    "                axes[0].imshow(img)\n",
    "                axes[0].set_title('Input')\n",
    "                axes[0].axis('off')\n",
    "                \n",
    "                # Target\n",
    "                target = test_sample['target_views'][0].cpu().permute(1, 2, 0).numpy()\n",
    "                target = (target * 0.5 + 0.5).clip(0, 1)\n",
    "                axes[1].imshow(target)\n",
    "                axes[1].set_title('Target')\n",
    "                axes[1].axis('off')\n",
    "                \n",
    "                # Rendered\n",
    "                rendered = output['rendered_image'][0].cpu().permute(1, 2, 0).numpy()\n",
    "                rendered = (rendered * 0.5 + 0.5).clip(0, 1)\n",
    "                axes[2].imshow(rendered)\n",
    "                axes[2].set_title(f'Rendered (Epoch {epoch})')\n",
    "                axes[2].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(SAVE_DIR, f'progress_epoch_{epoch}.png'), dpi=150)\n",
    "                plt.close()\n",
    "            model.train()\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best Loss: {best_loss:.4f}\")\n",
    "print(f\"Models saved to: {SAVE_DIR}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save loss history\n",
    "train_losses = {'total': train_loss_history, 'mse': mse_loss_history}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4f00d",
   "metadata": {},
   "source": [
    "## üìä B∆∞·ªõc 7: Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training loss\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total loss\n",
    "axes[0].plot(train_losses['total'], label='Total Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss (Total)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MSE loss\n",
    "axes[1].plot(train_losses['mse'], label='MSE Loss', color='orange', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Training Loss (MSE)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, 'training_loss.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Loss curves saved to {SAVE_DIR}/training_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e378d75",
   "metadata": {},
   "source": [
    "## üé® B∆∞·ªõc 8: Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a548d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test v·ªõi m·ªôt sample\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get a test sample\n",
    "    test_sample = train_dataset[0]\n",
    "    test_image = test_sample['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(test_image, return_intermediate=True)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Input\n",
    "    img = test_image[0].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img * 0.5 + 0.5).clip(0, 1)\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Input View')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Rendered output\n",
    "    rendered = output['rendered_image'][0].cpu().permute(1, 2, 0).numpy()\n",
    "    rendered = (rendered * 0.5 + 0.5).clip(0, 1)\n",
    "    axes[1].imshow(rendered)\n",
    "    axes[1].set_title('Rendered Output')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'test_output.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"‚úì Test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a3698",
   "metadata": {},
   "source": [
    "## üíæ B∆∞·ªõc 9: Load Model ƒê√£ Train (ƒê·ªÉ D√πng Sau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ff576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function ƒë·ªÉ load model ƒë√£ train\n",
    "def load_trained_model(checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load model ƒë√£ train t·ª´ checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file .pth\n",
    "        device: 'cuda' ho·∫∑c 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        model: Model ƒë√£ load weights\n",
    "        config: Training config\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Get config\n",
    "    config = checkpoint.get('config', {'triplane_channels': 32})\n",
    "    \n",
    "    # Create model\n",
    "    model = WaveletTriplaneDiffusion(\n",
    "        triplane_channels=config.get('triplane_channels', 32)\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úì Loaded model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"  Loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "# Example: Load best model\n",
    "best_model_path = os.path.join(SAVE_DIR, 'wavelet_triplane_best.pth')\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    loaded_model, loaded_config = load_trained_model(best_model_path, device=device)\n",
    "    print(\"\\n‚úì Model s·∫µn s√†ng ƒë·ªÉ inference!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ch∆∞a c√≥ model - h√£y train tr∆∞·ªõc!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672185d6",
   "metadata": {},
   "source": [
    "## üéØ B∆∞·ªõc 10: Inference v·ªõi Model ƒê√£ Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function ƒë·ªÉ generate 3D t·ª´ ·∫£nh\n",
    "@torch.no_grad()\n",
    "def generate_3d(model, input_image, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate 3D representation t·ª´ single image.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        input_image: PIL Image ho·∫∑c tensor (1, 3, 256, 256)\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        outputs: Dict ch·ª©a triplane v√† rendered image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess if PIL Image\n",
    "    if not isinstance(input_image, torch.Tensor):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        input_image = transform(input_image).unsqueeze(0)\n",
    "    \n",
    "    input_image = input_image.to(device)\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model(input_image, return_intermediate=True)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Test inference\n",
    "if os.path.exists(best_model_path):\n",
    "    test_img = torch.randn(1, 3, 256, 256).to(device)\n",
    "    result = generate_3d(loaded_model, test_img, device=device)\n",
    "    \n",
    "    print(\"Inference results:\")\n",
    "    print(f\"  Triplane shape: {result['triplane_highres'].shape}\")\n",
    "    print(f\"  Rendered image shape: {result['rendered_image'].shape}\")\n",
    "    print(\"\\n‚úì Inference successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b3398",
   "metadata": {},
   "source": [
    "## üì• B∆∞·ªõc 11: Download Model v·ªÅ M√°y (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model v·ªÅ m√°y local\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading trained model...\")\n",
    "files.download(best_model_path)\n",
    "print(\"‚úì Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9087e25",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "### ‚úÖ B·∫°n ƒë√£ ho√†n th√†nh:\n",
    "\n",
    "1. ‚úÖ Setup m√¥i tr∆∞·ªùng tr√™n Colab\n",
    "2. ‚úÖ T·∫£i dataset t·ª± ƒë·ªông\n",
    "3. ‚úÖ Train model v·ªõi GPU mi·ªÖn ph√≠\n",
    "4. ‚úÖ L∆∞u model v√†o Google Drive\n",
    "5. ‚úÖ Test v√† visualize k·∫øt qu·∫£\n",
    "\n",
    "### üìÇ Files ƒë√£ l∆∞u:\n",
    "\n",
    "```\n",
    "/content/drive/MyDrive/WaveletTriplane/\n",
    "‚îú‚îÄ‚îÄ wavelet_triplane_best.pth          # Best model\n",
    "‚îú‚îÄ‚îÄ wavelet_triplane_epoch_*.pth       # Checkpoints\n",
    "‚îú‚îÄ‚îÄ training_loss.png                  # Loss curve\n",
    "‚îî‚îÄ‚îÄ test_output.png                    # Test results\n",
    "```\n",
    "\n",
    "### üéØ S·ª≠ d·ª•ng sau n√†y:\n",
    "\n",
    "```python\n",
    "# Load model\n",
    "model, config = load_trained_model('path/to/best.pth')\n",
    "\n",
    "# Generate 3D\n",
    "result = generate_3d(model, your_image)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Ch√∫c m·ª´ng! B·∫°n ƒë√£ c√≥ model ri√™ng c·ªßa m√¨nh!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
