{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033ee588",
   "metadata": {},
   "source": [
    "## üì¶ B∆∞·ªõc 1: C√†i ƒê·∫∑t v√† Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404655c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b11118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install einops tqdm matplotlib pillow\n",
    "!pip install trimesh[easy] pytorch3d objaverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda64389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive ƒë·ªÉ l∆∞u model\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c l∆∞u model\n",
    "!mkdir -p /content/drive/MyDrive/WaveletTriplane\n",
    "\n",
    "SAVE_DIR = '/content/drive/MyDrive/WaveletTriplane'\n",
    "print(f\"Model s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae1cf2",
   "metadata": {},
   "source": [
    "## üì• B∆∞·ªõc 2: T·∫£i Dataset\n",
    "\n",
    "**L·ª±a ch·ªçn dataset:**\n",
    "- **Option A**: ShapeNet (nh·ªè, nhanh, d·ªÖ)\n",
    "- **Option B**: Objaverse (l·ªõn, ƒëa d·∫°ng, ch·∫≠m h∆°n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Download ShapeNet subset (simplified version)\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Download small ShapeNet subset (chairs - ~500MB)\n",
    "DATASET_DIR = '/content/shapenet_data'\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Downloading ShapeNet chairs subset...\")\n",
    "# S·ª≠ d·ª•ng ShapeNet Core v2 chairs (ho·∫∑c b·∫°n c√≥ th·ªÉ thay b·∫±ng link kh√°c)\n",
    "# Link demo - thay b·∫±ng link th·ª±c t·∫ø c·ªßa b·∫°n\n",
    "# !wget -P /content/shapenet_data \"YOUR_SHAPENET_LINK_HERE\"\n",
    "\n",
    "# T·∫°m th·ªùi t·∫°o dummy data ƒë·ªÉ demo\n",
    "print(\"‚ö†Ô∏è ƒêang d√πng dummy data - thay b·∫±ng ShapeNet th·ª±c n·∫øu c√≥ link\")\n",
    "!mkdir -p /content/shapenet_data/chairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc4a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Download Objaverse samples (automated)\n",
    "import objaverse\n",
    "\n",
    "# T·∫£i 100 objects ƒë·∫ßu ti√™n t·ª´ Objaverse\n",
    "print(\"Downloading Objaverse objects...\")\n",
    "uids = objaverse.load_uids()[:100]  # L·∫•y 100 objects\n",
    "\n",
    "objects = objaverse.load_objects(\n",
    "    uids=uids,\n",
    "    download_processes=4\n",
    ")\n",
    "\n",
    "print(f\"Downloaded {len(objects)} objects to {objaverse._VERSIONED_PATH}\")\n",
    "DATASET_DIR = objaverse._VERSIONED_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7bf89f",
   "metadata": {},
   "source": [
    "## üèóÔ∏è B∆∞·ªõc 3: Copy Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone ho·∫∑c upload code c·ªßa b·∫°n\n",
    "# N·∫øu code tr√™n GitHub:\n",
    "# !git clone YOUR_GITHUB_REPO\n",
    "\n",
    "# Ho·∫∑c t·∫£i tr·ª±c ti·∫øp t·ª´ Google Drive n·∫øu ƒë√£ upload\n",
    "# !cp -r /content/drive/MyDrive/WaveletTriplaneDiff /content/\n",
    "\n",
    "# Ho·∫∑c t·∫°o file tr·ª±c ti·∫øp trong notebook (s·∫Ω l√†m ·ªü cell ti·∫øp theo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54723543",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wavelet_triplane_diffusion.py\n",
    "\n",
    "# Paste to√†n b·ªô n·ªôi dung file wavelet_triplane_diffusion.py v√†o ƒë√¢y\n",
    "# Ho·∫∑c upload file t·ª´ m√°y local\n",
    "\n",
    "# ƒê·ªÉ ti·∫øt ki·ªám space, t√¥i s·∫Ω d√πng l·ªánh upload\n",
    "print(\"‚ö†Ô∏è Vui l√≤ng upload file wavelet_triplane_diffusion.py v√†o Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2926f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file t·ª´ m√°y local\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Upload file wavelet_triplane_diffusion.py t·ª´ m√°y t√≠nh c·ªßa b·∫°n:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Ki·ªÉm tra\n",
    "!ls -lh wavelet_triplane_diffusion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa7b1c",
   "metadata": {},
   "source": [
    "## üéØ B∆∞·ªõc 4: T·∫°o Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db63c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MultiViewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset cho 3D objects v·ªõi multi-view rendering.\n",
    "    Gi·∫£ ƒë·ªãnh: m·ªói object c√≥ nhi·ªÅu view ƒë√£ render s·∫µn.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, num_views=4, image_size=256):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.num_views = num_views\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Transform\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        # T√¨m t·∫•t c·∫£ objects\n",
    "        self.objects = []\n",
    "        if os.path.exists(data_dir):\n",
    "            for obj_dir in Path(data_dir).iterdir():\n",
    "                if obj_dir.is_dir():\n",
    "                    self.objects.append(obj_dir)\n",
    "        \n",
    "        # N·∫øu kh√¥ng c√≥ data th·∫≠t, t·∫°o synthetic data\n",
    "        if len(self.objects) == 0:\n",
    "            print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y data th·∫≠t - s·ª≠ d·ª•ng synthetic data\")\n",
    "            self.use_synthetic = True\n",
    "            self.num_samples = 1000\n",
    "        else:\n",
    "            self.use_synthetic = False\n",
    "            print(f\"‚úì T√¨m th·∫•y {len(self.objects)} objects\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples if self.use_synthetic else len(self.objects)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.use_synthetic:\n",
    "            # Synthetic data\n",
    "            input_view = torch.randn(3, self.image_size, self.image_size)\n",
    "            target_views = torch.randn(self.num_views, 3, 64, 64)\n",
    "        else:\n",
    "            # Load real data\n",
    "            obj_dir = self.objects[idx]\n",
    "            \n",
    "            # Gi·∫£ ƒë·ªãnh c√≥ file view_0.png, view_1.png, etc.\n",
    "            input_path = obj_dir / 'view_0.png'\n",
    "            \n",
    "            if input_path.exists():\n",
    "                input_view = Image.open(input_path).convert('RGB')\n",
    "                input_view = self.transform(input_view)\n",
    "            else:\n",
    "                input_view = torch.randn(3, self.image_size, self.image_size)\n",
    "            \n",
    "            # Load target views\n",
    "            target_views = []\n",
    "            for i in range(1, self.num_views + 1):\n",
    "                view_path = obj_dir / f'view_{i}.png'\n",
    "                if view_path.exists():\n",
    "                    view = Image.open(view_path).convert('RGB')\n",
    "                    view = self.transform(view)\n",
    "                    view = F.interpolate(view.unsqueeze(0), size=64, mode='bilinear')[0]\n",
    "                else:\n",
    "                    view = torch.randn(3, 64, 64)\n",
    "                target_views.append(view)\n",
    "            \n",
    "            target_views = torch.stack(target_views)\n",
    "        \n",
    "        return {\n",
    "            'image': input_view,\n",
    "            'target_views': target_views,\n",
    "        }\n",
    "\n",
    "# Test dataset\n",
    "dataset = MultiViewDataset(DATASET_DIR)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "sample = dataset[0]\n",
    "print(f\"Input shape: {sample['image'].shape}\")\n",
    "print(f\"Target views shape: {sample['target_views'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da7b13",
   "metadata": {},
   "source": [
    "## üèãÔ∏è B∆∞·ªõc 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from wavelet_triplane_diffusion import WaveletTriplaneDiffusion\n",
    "\n",
    "# Training config\n",
    "CONFIG = {\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'triplane_channels': 32,\n",
    "    'num_timesteps': 1000,\n",
    "    'save_every': 5,  # Save checkpoint m·ªói 5 epochs\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beff08e",
   "metadata": {},
   "source": [
    "## üöÄ B∆∞·ªõc 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create dataloader\n",
    "train_dataset = MultiViewDataset(DATASET_DIR)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Create model\n",
    "device = CONFIG['device']\n",
    "model = WaveletTriplaneDiffusion(triplane_channels=CONFIG['triplane_channels'])\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['num_epochs'])\n",
    "\n",
    "# Loss tracking\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        images = batch['image'].to(device)\n",
    "        target_views = batch['target_views'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, return_intermediate=True)\n",
    "        \n",
    "        # Simple rendering loss\n",
    "        rendered = outputs['rendered_image']\n",
    "        target = target_views[:, 0]  # First target view\n",
    "        \n",
    "        loss = F.mse_loss(rendered, target)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2733e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "    # Train\n",
    "    avg_loss = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # Log\n",
    "    print(f\"\\nEpoch {epoch}/{CONFIG['num_epochs']}\")\n",
    "    print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        save_path = os.path.join(SAVE_DIR, 'wavelet_triplane_best.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "            'config': CONFIG,\n",
    "        }, save_path)\n",
    "        print(f\"  ‚úì Saved best model to {save_path}\")\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if epoch % CONFIG['save_every'] == 0:\n",
    "        save_path = os.path.join(SAVE_DIR, f'wavelet_triplane_epoch_{epoch}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'config': CONFIG,\n",
    "        }, save_path)\n",
    "        print(f\"  ‚úì Saved checkpoint to {save_path}\")\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best Loss: {best_loss:.4f}\")\n",
    "print(f\"Models saved to: {SAVE_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4f00d",
   "metadata": {},
   "source": [
    "## üìä B∆∞·ªõc 7: Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Wavelet Triplane Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(SAVE_DIR, 'training_loss.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss curve saved to {SAVE_DIR}/training_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e378d75",
   "metadata": {},
   "source": [
    "## üé® B∆∞·ªõc 8: Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a548d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test v·ªõi m·ªôt sample\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get a test sample\n",
    "    test_sample = train_dataset[0]\n",
    "    test_image = test_sample['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(test_image, return_intermediate=True)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Input\n",
    "    img = test_image[0].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img * 0.5 + 0.5).clip(0, 1)\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Input View')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Rendered output\n",
    "    rendered = output['rendered_image'][0].cpu().permute(1, 2, 0).numpy()\n",
    "    rendered = (rendered * 0.5 + 0.5).clip(0, 1)\n",
    "    axes[1].imshow(rendered)\n",
    "    axes[1].set_title('Rendered Output')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'test_output.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"‚úì Test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a3698",
   "metadata": {},
   "source": [
    "## üíæ B∆∞·ªõc 9: Load Model ƒê√£ Train (ƒê·ªÉ D√πng Sau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ff576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function ƒë·ªÉ load model ƒë√£ train\n",
    "def load_trained_model(checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load model ƒë√£ train t·ª´ checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file .pth\n",
    "        device: 'cuda' ho·∫∑c 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        model: Model ƒë√£ load weights\n",
    "        config: Training config\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Get config\n",
    "    config = checkpoint.get('config', {'triplane_channels': 32})\n",
    "    \n",
    "    # Create model\n",
    "    model = WaveletTriplaneDiffusion(\n",
    "        triplane_channels=config.get('triplane_channels', 32)\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úì Loaded model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"  Loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "# Example: Load best model\n",
    "best_model_path = os.path.join(SAVE_DIR, 'wavelet_triplane_best.pth')\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    loaded_model, loaded_config = load_trained_model(best_model_path, device=device)\n",
    "    print(\"\\n‚úì Model s·∫µn s√†ng ƒë·ªÉ inference!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ch∆∞a c√≥ model - h√£y train tr∆∞·ªõc!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672185d6",
   "metadata": {},
   "source": [
    "## üéØ B∆∞·ªõc 10: Inference v·ªõi Model ƒê√£ Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function ƒë·ªÉ generate 3D t·ª´ ·∫£nh\n",
    "@torch.no_grad()\n",
    "def generate_3d(model, input_image, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate 3D representation t·ª´ single image.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        input_image: PIL Image ho·∫∑c tensor (1, 3, 256, 256)\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        outputs: Dict ch·ª©a triplane v√† rendered image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess if PIL Image\n",
    "    if not isinstance(input_image, torch.Tensor):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        input_image = transform(input_image).unsqueeze(0)\n",
    "    \n",
    "    input_image = input_image.to(device)\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model(input_image, return_intermediate=True)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Test inference\n",
    "if os.path.exists(best_model_path):\n",
    "    test_img = torch.randn(1, 3, 256, 256).to(device)\n",
    "    result = generate_3d(loaded_model, test_img, device=device)\n",
    "    \n",
    "    print(\"Inference results:\")\n",
    "    print(f\"  Triplane shape: {result['triplane_highres'].shape}\")\n",
    "    print(f\"  Rendered image shape: {result['rendered_image'].shape}\")\n",
    "    print(\"\\n‚úì Inference successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b3398",
   "metadata": {},
   "source": [
    "## üì• B∆∞·ªõc 11: Download Model v·ªÅ M√°y (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model v·ªÅ m√°y local\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading trained model...\")\n",
    "files.download(best_model_path)\n",
    "print(\"‚úì Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9087e25",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "### ‚úÖ B·∫°n ƒë√£ ho√†n th√†nh:\n",
    "\n",
    "1. ‚úÖ Setup m√¥i tr∆∞·ªùng tr√™n Colab\n",
    "2. ‚úÖ T·∫£i dataset t·ª± ƒë·ªông\n",
    "3. ‚úÖ Train model v·ªõi GPU mi·ªÖn ph√≠\n",
    "4. ‚úÖ L∆∞u model v√†o Google Drive\n",
    "5. ‚úÖ Test v√† visualize k·∫øt qu·∫£\n",
    "\n",
    "### üìÇ Files ƒë√£ l∆∞u:\n",
    "\n",
    "```\n",
    "/content/drive/MyDrive/WaveletTriplane/\n",
    "‚îú‚îÄ‚îÄ wavelet_triplane_best.pth          # Best model\n",
    "‚îú‚îÄ‚îÄ wavelet_triplane_epoch_*.pth       # Checkpoints\n",
    "‚îú‚îÄ‚îÄ training_loss.png                  # Loss curve\n",
    "‚îî‚îÄ‚îÄ test_output.png                    # Test results\n",
    "```\n",
    "\n",
    "### üéØ S·ª≠ d·ª•ng sau n√†y:\n",
    "\n",
    "```python\n",
    "# Load model\n",
    "model, config = load_trained_model('path/to/best.pth')\n",
    "\n",
    "# Generate 3D\n",
    "result = generate_3d(model, your_image)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Ch√∫c m·ª´ng! B·∫°n ƒë√£ c√≥ model ri√™ng c·ªßa m√¨nh!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
