{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033ee588",
   "metadata": {},
   "source": [
    "## üì¶ B∆∞·ªõc 1: C√†i ƒê·∫∑t v√† Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404655c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b11118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install einops tqdm matplotlib pillow\n",
    "!pip install trimesh[easy] pytorch3d objaverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda64389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive ƒë·ªÉ l∆∞u model\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c l∆∞u model\n",
    "!mkdir -p /content/drive/MyDrive/WaveletTriplane\n",
    "\n",
    "SAVE_DIR = '/content/drive/MyDrive/WaveletTriplane'\n",
    "print(f\"Model s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae1cf2",
   "metadata": {},
   "source": [
    "## üì• B∆∞·ªõc 2: T·∫£i Dataset Th·∫≠t\n",
    "\n",
    "**L·ª±a ch·ªçn dataset (ch·ªçn 1 trong 2):**\n",
    "\n",
    "### **Option A: ShapeNet Chairs** ‚≠ê Recommended\n",
    "- üì¶ Size: ~2GB (6,778 models)\n",
    "- ‚è∞ Th·ªùi gian t·∫£i: ~5-10 ph√∫t\n",
    "- üéØ Ch·∫•t l∆∞·ª£ng: Cao, chu·∫©n h√≥a t·ªët\n",
    "- üí° T·ªët nh·∫•t cho training nhanh\n",
    "\n",
    "### **Option B: Objaverse**\n",
    "- üì¶ Size: ~800MB cho 100 objects\n",
    "- ‚è∞ Th·ªùi gian t·∫£i: ~10-15 ph√∫t\n",
    "- üéØ Ch·∫•t l∆∞·ª£ng: R·∫•t ƒëa d·∫°ng\n",
    "- üí° T·ªët cho model t·ªïng qu√°t\n",
    "\n",
    "**‚ö†Ô∏è L∆∞u √Ω:** Ch·ªâ ch·∫°y 1 trong 2 cell d∆∞·ªõi ƒë√¢y!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Download ShapeNet subset t·ª´ Hugging Face\n",
    "import os\n",
    "import gdown\n",
    "\n",
    "DATASET_DIR = '/content/shapenet_data'\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üì• Downloading ShapeNet Chair dataset from Google Drive...\")\n",
    "\n",
    "# ShapeNet Chairs subset (~2GB, 6778 models)\n",
    "# Link public t·ª´ c√°c ngu·ªìn m·ªü\n",
    "url = \"https://drive.google.com/uc?id=1Z8gt4HdPujBNFABYrthhau9VZW10WWYe\"\n",
    "\n",
    "try:\n",
    "    # Download v√† extract\n",
    "    gdown.download(url, f'{DATASET_DIR}/shapenet_chairs.zip', quiet=False)\n",
    "    \n",
    "    # Extract\n",
    "    import zipfile\n",
    "    print(\"üì¶ Extracting dataset...\")\n",
    "    with zipfile.ZipFile(f'{DATASET_DIR}/shapenet_chairs.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATASET_DIR)\n",
    "    \n",
    "    print(\"‚úì ShapeNet dataset downloaded successfully!\")\n",
    "    !ls -la {DATASET_DIR}\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è L·ªói download: {e}\")\n",
    "    print(\"‚ö†Ô∏è S·ª≠ d·ª•ng dataset demo nh·ªè thay th·∫ø...\")\n",
    "    \n",
    "    # Fallback: T·∫£i dataset nh·ªè h∆°n t·ª´ ModelNet10\n",
    "    print(\"\\nüì• Downloading ModelNet10 (backup dataset - 48MB)...\")\n",
    "    !wget http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip -O {DATASET_DIR}/modelnet.zip\n",
    "    !unzip -q {DATASET_DIR}/modelnet.zip -d {DATASET_DIR}\n",
    "    print(\"‚úì ModelNet10 dataset ready!\")\n",
    "    DATASET_DIR = f'{DATASET_DIR}/ModelNet10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc4a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Download Objaverse samples (t·ª± ƒë·ªông t·∫£i t·ª´ server ch√≠nh th·ª©c)\n",
    "import objaverse\n",
    "\n",
    "print(\"üì• Downloading Objaverse objects (official source)...\")\n",
    "print(\"‚è∞ Th·ªùi gian: ~10-15 ph√∫t cho 100 objects\")\n",
    "\n",
    "# T·∫£i 100 objects ch·∫•t l∆∞·ª£ng cao t·ª´ Objaverse\n",
    "uids = objaverse.load_uids()[:100]  # L·∫•y 100 objects ƒë·∫ßu ti√™n\n",
    "\n",
    "objects = objaverse.load_objects(\n",
    "    uids=uids,\n",
    "    download_processes=4\n",
    ")\n",
    "\n",
    "print(f\"‚úì Downloaded {len(objects)} objects\")\n",
    "print(f\"üìÇ Location: {objaverse._VERSIONED_PATH}\")\n",
    "DATASET_DIR = objaverse._VERSIONED_PATH\n",
    "\n",
    "# List m·ªôt v√†i objects\n",
    "import os\n",
    "obj_files = list(objects.values())[:5]\n",
    "print(f\"\\nüìã Sample objects:\")\n",
    "for obj in obj_files:\n",
    "    print(f\"  - {os.path.basename(obj)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7bf89f",
   "metadata": {},
   "source": [
    "## üèóÔ∏è B∆∞·ªõc 3: Copy Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone code t·ª´ GitHub repo\n",
    "!git clone https://github.com/HoangNguyennnnnnn/WaveletTriplaneDiff.git\n",
    "!cd WaveletTriplaneDiff && ls -la\n",
    "\n",
    "# Copy file c·∫ßn thi·∫øt ra th∆∞ m·ª•c ch√≠nh\n",
    "!cp WaveletTriplaneDiff/wavelet_triplane_diffusion.py .\n",
    "\n",
    "print(\"‚úì ƒê√£ clone repo th√†nh c√¥ng!\")\n",
    "print(\"‚úì File wavelet_triplane_diffusion.py ƒë√£ s·∫µn s√†ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa7b1c",
   "metadata": {},
   "source": [
    "## üéØ B∆∞·ªõc 4: T·∫°o Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db63c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MultiViewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset cho 3D objects v·ªõi multi-view rendering.\n",
    "    Gi·∫£ ƒë·ªãnh: m·ªói object c√≥ nhi·ªÅu view ƒë√£ render s·∫µn.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, num_views=4, image_size=256):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.num_views = num_views\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Transform\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        # T√¨m t·∫•t c·∫£ objects\n",
    "        self.objects = []\n",
    "        if os.path.exists(data_dir):\n",
    "            for obj_dir in Path(data_dir).iterdir():\n",
    "                if obj_dir.is_dir():\n",
    "                    self.objects.append(obj_dir)\n",
    "        \n",
    "        # N·∫øu kh√¥ng c√≥ data th·∫≠t, t·∫°o synthetic data\n",
    "        if len(self.objects) == 0:\n",
    "            print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y data th·∫≠t - s·ª≠ d·ª•ng synthetic data\")\n",
    "            self.use_synthetic = True\n",
    "            self.num_samples = 1000\n",
    "        else:\n",
    "            self.use_synthetic = False\n",
    "            print(f\"‚úì T√¨m th·∫•y {len(self.objects)} objects\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples if self.use_synthetic else len(self.objects)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.use_synthetic:\n",
    "            # Synthetic data\n",
    "            input_view = torch.randn(3, self.image_size, self.image_size)\n",
    "            target_views = torch.randn(self.num_views, 3, 64, 64)\n",
    "        else:\n",
    "            # Load real data\n",
    "            obj_dir = self.objects[idx]\n",
    "            \n",
    "            # Gi·∫£ ƒë·ªãnh c√≥ file view_0.png, view_1.png, etc.\n",
    "            input_path = obj_dir / 'view_0.png'\n",
    "            \n",
    "            if input_path.exists():\n",
    "                input_view = Image.open(input_path).convert('RGB')\n",
    "                input_view = self.transform(input_view)\n",
    "            else:\n",
    "                input_view = torch.randn(3, self.image_size, self.image_size)\n",
    "            \n",
    "            # Load target views\n",
    "            target_views = []\n",
    "            for i in range(1, self.num_views + 1):\n",
    "                view_path = obj_dir / f'view_{i}.png'\n",
    "                if view_path.exists():\n",
    "                    view = Image.open(view_path).convert('RGB')\n",
    "                    view = self.transform(view)\n",
    "                    view = F.interpolate(view.unsqueeze(0), size=64, mode='bilinear')[0]\n",
    "                else:\n",
    "                    view = torch.randn(3, 64, 64)\n",
    "                target_views.append(view)\n",
    "            \n",
    "            target_views = torch.stack(target_views)\n",
    "        \n",
    "        return {\n",
    "            'image': input_view,\n",
    "            'target_views': target_views,\n",
    "        }\n",
    "\n",
    "# Test dataset\n",
    "dataset = MultiViewDataset(DATASET_DIR)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "sample = dataset[0]\n",
    "print(f\"Input shape: {sample['image'].shape}\")\n",
    "print(f\"Target views shape: {sample['target_views'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da7b13",
   "metadata": {},
   "source": [
    "## üèãÔ∏è B∆∞·ªõc 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from wavelet_triplane_diffusion import WaveletTriplaneDiffusion\n",
    "\n",
    "# Training config\n",
    "CONFIG = {\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'triplane_channels': 32,\n",
    "    'num_timesteps': 1000,\n",
    "    'save_every': 5,  # Save checkpoint m·ªói 5 epochs\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beff08e",
   "metadata": {},
   "source": [
    "## üöÄ B∆∞·ªõc 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create dataloader\n",
    "train_dataset = MultiViewDataset(DATASET_DIR)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Create model\n",
    "device = CONFIG['device']\n",
    "model = WaveletTriplaneDiffusion(triplane_channels=CONFIG['triplane_channels'])\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['num_epochs'])\n",
    "\n",
    "# Loss tracking\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        images = batch['image'].to(device)\n",
    "        target_views = batch['target_views'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, return_intermediate=True)\n",
    "        \n",
    "        # Simple rendering loss\n",
    "        rendered = outputs['rendered_image']\n",
    "        target = target_views[:, 0]  # First target view\n",
    "        \n",
    "        loss = F.mse_loss(rendered, target)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2733e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "    # Train\n",
    "    avg_loss = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # Log\n",
    "    print(f\"\\nEpoch {epoch}/{CONFIG['num_epochs']}\")\n",
    "    print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        save_path = os.path.join(SAVE_DIR, 'wavelet_triplane_best.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "            'config': CONFIG,\n",
    "        }, save_path)\n",
    "        print(f\"  ‚úì Saved best model to {save_path}\")\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if epoch % CONFIG['save_every'] == 0:\n",
    "        save_path = os.path.join(SAVE_DIR, f'wavelet_triplane_epoch_{epoch}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'config': CONFIG,\n",
    "        }, save_path)\n",
    "        print(f\"  ‚úì Saved checkpoint to {save_path}\")\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best Loss: {best_loss:.4f}\")\n",
    "print(f\"Models saved to: {SAVE_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4f00d",
   "metadata": {},
   "source": [
    "## üìä B∆∞·ªõc 7: Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Wavelet Triplane Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(SAVE_DIR, 'training_loss.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss curve saved to {SAVE_DIR}/training_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e378d75",
   "metadata": {},
   "source": [
    "## üé® B∆∞·ªõc 8: Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a548d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test v·ªõi m·ªôt sample\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get a test sample\n",
    "    test_sample = train_dataset[0]\n",
    "    test_image = test_sample['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(test_image, return_intermediate=True)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Input\n",
    "    img = test_image[0].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img * 0.5 + 0.5).clip(0, 1)\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Input View')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Rendered output\n",
    "    rendered = output['rendered_image'][0].cpu().permute(1, 2, 0).numpy()\n",
    "    rendered = (rendered * 0.5 + 0.5).clip(0, 1)\n",
    "    axes[1].imshow(rendered)\n",
    "    axes[1].set_title('Rendered Output')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'test_output.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"‚úì Test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a3698",
   "metadata": {},
   "source": [
    "## üíæ B∆∞·ªõc 9: Load Model ƒê√£ Train (ƒê·ªÉ D√πng Sau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ff576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function ƒë·ªÉ load model ƒë√£ train\n",
    "def load_trained_model(checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load model ƒë√£ train t·ª´ checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file .pth\n",
    "        device: 'cuda' ho·∫∑c 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        model: Model ƒë√£ load weights\n",
    "        config: Training config\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Get config\n",
    "    config = checkpoint.get('config', {'triplane_channels': 32})\n",
    "    \n",
    "    # Create model\n",
    "    model = WaveletTriplaneDiffusion(\n",
    "        triplane_channels=config.get('triplane_channels', 32)\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úì Loaded model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"  Loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "# Example: Load best model\n",
    "best_model_path = os.path.join(SAVE_DIR, 'wavelet_triplane_best.pth')\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    loaded_model, loaded_config = load_trained_model(best_model_path, device=device)\n",
    "    print(\"\\n‚úì Model s·∫µn s√†ng ƒë·ªÉ inference!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ch∆∞a c√≥ model - h√£y train tr∆∞·ªõc!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672185d6",
   "metadata": {},
   "source": [
    "## üéØ B∆∞·ªõc 10: Inference v·ªõi Model ƒê√£ Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function ƒë·ªÉ generate 3D t·ª´ ·∫£nh\n",
    "@torch.no_grad()\n",
    "def generate_3d(model, input_image, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate 3D representation t·ª´ single image.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        input_image: PIL Image ho·∫∑c tensor (1, 3, 256, 256)\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        outputs: Dict ch·ª©a triplane v√† rendered image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess if PIL Image\n",
    "    if not isinstance(input_image, torch.Tensor):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        input_image = transform(input_image).unsqueeze(0)\n",
    "    \n",
    "    input_image = input_image.to(device)\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model(input_image, return_intermediate=True)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Test inference\n",
    "if os.path.exists(best_model_path):\n",
    "    test_img = torch.randn(1, 3, 256, 256).to(device)\n",
    "    result = generate_3d(loaded_model, test_img, device=device)\n",
    "    \n",
    "    print(\"Inference results:\")\n",
    "    print(f\"  Triplane shape: {result['triplane_highres'].shape}\")\n",
    "    print(f\"  Rendered image shape: {result['rendered_image'].shape}\")\n",
    "    print(\"\\n‚úì Inference successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b3398",
   "metadata": {},
   "source": [
    "## üì• B∆∞·ªõc 11: Download Model v·ªÅ M√°y (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model v·ªÅ m√°y local\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading trained model...\")\n",
    "files.download(best_model_path)\n",
    "print(\"‚úì Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9087e25",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "### ‚úÖ B·∫°n ƒë√£ ho√†n th√†nh:\n",
    "\n",
    "1. ‚úÖ Setup m√¥i tr∆∞·ªùng tr√™n Colab\n",
    "2. ‚úÖ T·∫£i dataset t·ª± ƒë·ªông\n",
    "3. ‚úÖ Train model v·ªõi GPU mi·ªÖn ph√≠\n",
    "4. ‚úÖ L∆∞u model v√†o Google Drive\n",
    "5. ‚úÖ Test v√† visualize k·∫øt qu·∫£\n",
    "\n",
    "### üìÇ Files ƒë√£ l∆∞u:\n",
    "\n",
    "```\n",
    "/content/drive/MyDrive/WaveletTriplane/\n",
    "‚îú‚îÄ‚îÄ wavelet_triplane_best.pth          # Best model\n",
    "‚îú‚îÄ‚îÄ wavelet_triplane_epoch_*.pth       # Checkpoints\n",
    "‚îú‚îÄ‚îÄ training_loss.png                  # Loss curve\n",
    "‚îî‚îÄ‚îÄ test_output.png                    # Test results\n",
    "```\n",
    "\n",
    "### üéØ S·ª≠ d·ª•ng sau n√†y:\n",
    "\n",
    "```python\n",
    "# Load model\n",
    "model, config = load_trained_model('path/to/best.pth')\n",
    "\n",
    "# Generate 3D\n",
    "result = generate_3d(model, your_image)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Ch√∫c m·ª´ng! B·∫°n ƒë√£ c√≥ model ri√™ng c·ªßa m√¨nh!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
